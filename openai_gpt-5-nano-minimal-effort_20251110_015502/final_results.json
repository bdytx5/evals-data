{
  "model": "openai/gpt-5-nano-minimal-effort",
  "timestamp": "2025-11-10T01:56:59.680417",
  "num_samples": 23,
  "results": [
    {
      "dataset": "AIME2024",
      "total_samples": 23,
      "correct": 9,
      "accuracy": 0.391304347826087,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-4f55-7371-85b3-ccb84f2e2412%3FhideTraceTree%3D1"
    },
    {
      "dataset": "AIME2025",
      "total_samples": 23,
      "correct": 7,
      "accuracy": 0.30434782608695654,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-5070-742f-81aa-cd7100d1d191%3FhideTraceTree%3D1"
    },
    {
      "dataset": "HMMTFeb2024",
      "total_samples": 23,
      "correct": 7,
      "accuracy": 0.30434782608695654,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-5198-7274-a961-95fa7e6d257a%3FhideTraceTree%3D1"
    },
    {
      "dataset": "HMMTFeb2025",
      "total_samples": 23,
      "correct": 3,
      "accuracy": 0.13043478260869565,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-5316-72bc-b87c-4360a7c730b0%3FhideTraceTree%3D1"
    },
    {
      "dataset": "CMIMC2025",
      "total_samples": 23,
      "correct": 0,
      "accuracy": 0.0,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-54cc-7cb2-93e5-65ee0618d7dd%3FhideTraceTree%3D1"
    },
    {
      "dataset": "BRUMO2025",
      "total_samples": 23,
      "correct": 13,
      "accuracy": 0.5652173913043478,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-55d6-7d2f-aa23-70e572aa2b8c%3FhideTraceTree%3D1"
    },
    {
      "dataset": "MMMU",
      "total_samples": 23,
      "correct": 14,
      "accuracy": 0.6086956521739131,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-572a-7995-86fd-f5d82ec8262a%3FhideTraceTree%3D1"
    },
    {
      "dataset": "MMMU-Pro",
      "total_samples": 23,
      "correct": 11,
      "accuracy": 0.4782608695652174,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-5a6f-7c70-8c99-1969cfdee068%3FhideTraceTree%3D1"
    },
    {
      "dataset": "CharXiv",
      "total_samples": 23,
      "correct": 13,
      "accuracy": 0.5652173913043478,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-687a-7735-ba15-340f0d576b7a%3FhideTraceTree%3D1"
    },
    {
      "dataset": "GPQA-Diamond",
      "total_samples": 23,
      "correct": 7,
      "accuracy": 0.30434782608695654,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-703a-7f32-8f41-b15def2ad080%3FhideTraceTree%3D1"
    },
    {
      "dataset": "HLE",
      "total_samples": 23,
      "correct": 0,
      "accuracy": 0.0,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-e21e-7349-b6b0-7edaf957b203%3FhideTraceTree%3D1"
    },
    {
      "dataset": "MMLU-Pro",
      "total_samples": 23,
      "correct": 4,
      "accuracy": 0.17391304347826086,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-e352-7f81-92a2-d87c9d3fd2e1%3FhideTraceTree%3D1"
    },
    {
      "dataset": "SimpleQA",
      "total_samples": 23,
      "correct": 2,
      "accuracy": 0.08695652173913043,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-e4ed-78c4-aeab-0600e6d2ee1e%3FhideTraceTree%3D1"
    },
    {
      "dataset": "COLLIE",
      "total_samples": 23,
      "correct": 10,
      "accuracy": 0.43478260869565216,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-e662-7542-b5e4-90e357eb90e7%3FhideTraceTree%3D1"
    },
    {
      "dataset": "CodeContests",
      "total_samples": 23,
      "correct": 1,
      "accuracy": 0.043478260869565216,
      "weave_url": "https://wandb.ai/byyoung3/multi-dataset-evaluation/weave/evaluations?view=evaluations_default&peekPath=%2Fbyyoung3%2Fmulti-dataset-evaluation%2Fcalls%2F019a6cc3-e749-75db-b20f-1697d0cb11cc%3FhideTraceTree%3D1"
    }
  ]
}